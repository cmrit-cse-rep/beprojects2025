{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616c73f8",
   "metadata": {},
   "source": [
    "<h2>Extract Frames from video</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645a6dd-7076-4aea-b228-778fb7ed5dbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "vi = \"./vid/v1.mp4\"\n",
    "print(vi)\n",
    "vi = Path(vi)\n",
    "print(vi)\n",
    "print(vi.stem)\n",
    "\n",
    "directory = Path(f\"./data_3rd_{vi.stem}\")\n",
    "directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec10758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "vi = \"./vid/v1.mp4\"\n",
    "vi = Path(vi)\n",
    "frames_directory = f\"./data_3rd_{vi.stem}\"\n",
    "\n",
    "cap = cv2.VideoCapture(vi)\n",
    "\n",
    "directory = Path(frames_directory)\n",
    "directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "frame_no = 300\n",
    "\n",
    "frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "req_frames = [i for i in range(int(frame_count)) if i%1800 == 0 and i!=0]\n",
    "r = []\n",
    "for i in req_frames:\n",
    "    a,b,c = i-2,i-1,i\n",
    "    r.append(a)\n",
    "    r.append(b)\n",
    "    r.append(c)\n",
    "# print(req_frames,len(req_frames))\n",
    "print(r,len(r))\n",
    "frames=[]\n",
    "\n",
    "#Save Frames in order\n",
    "for j,i in enumerate(r,start=1):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    res, frame = cap.read()\n",
    "    if res:\n",
    "        name=f'./data_3rd_{vi.stem}/frame{j}_{str(i)}.jpg'\n",
    "        # frames.append(name)\n",
    "        frames.append(Path(name))\n",
    "        # print('creating'+name)\n",
    "        cv2.imwrite(name,frame)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "print(frames,len(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df08cdaa",
   "metadata": {},
   "source": [
    "<h2>Emotion detection function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a547f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "def detect_emotion_mtcnn(path,backend=\"mtcnn\"):\n",
    "  try:\n",
    "      analysis = DeepFace.analyze(\n",
    "          img_path=path,\n",
    "          actions=[\"emotion\"],\n",
    "          # detector_backend=\"facenet\" #(facenet)\n",
    "          detector_backend=backend,#(MTCNN)\n",
    "          enforce_detection=False,\n",
    "          expand_percentage=30\n",
    "      )\n",
    "\n",
    "      # print(\"Emotion Analysis:\")\n",
    "      # print(analysis[0][\"emotion\"])\n",
    "      # print(f\"Dominant Emotion: {analysis[0]['dominant_emotion']}\")\n",
    "      \n",
    "      next_dominant_emotion=analysis[0]['emotion']\n",
    "\n",
    "      return analysis[0]['dominant_emotion']\n",
    "\n",
    "  except Exception as e:\n",
    "      # print(f\"An error occurred: {e}\")\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53262e",
   "metadata": {},
   "source": [
    "<h2>Eye Gaze function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0233e9-c49a-428b-a9c9-6a3788f71c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719928b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "# Initialize Mediapipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=100, refine_landmarks=True, min_detection_confidence=0.2, min_tracking_confidence=0.3)\n",
    "\n",
    "def eye_aspect_ratio(eye_landmarks, landmarks):\n",
    "    A = np.linalg.norm(np.array(landmarks[1]) - np.array(landmarks[5]))\n",
    "    B = np.linalg.norm(np.array(landmarks[2]) - np.array(landmarks[4]))\n",
    "    C = np.linalg.norm(np.array(landmarks[0]) - np.array(landmarks[3]))\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "\n",
    "def detect_eye_pose(image):\n",
    "    \"\"\"\n",
    "    Detects eye pose based on the position of the iris.\n",
    "    Args:\n",
    "        image: Input image\n",
    "    Returns:\n",
    "        Annotated image with eye pose directions\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    results = face_mesh.process(rgb_image)\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        # print(\"No face detected.\")\n",
    "        return \"No face detected.\"\n",
    "\n",
    "    annotated_image = image.copy()\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "        # Define eye landmarks (Mediapipe indexes)\n",
    "        left_eye_indexes = [33, 159, 158, 133, 153, 145]  # Example keypoints for left eye\n",
    "        right_eye_indexes = [362, 380, 374, 263, 386, 385]  # Example keypoints for right eye\n",
    "\n",
    "\n",
    "\n",
    "        # left_eye_indexes = [463, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382, 362]\n",
    "        # right_eye_indexes = [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7]\n",
    "\n",
    "        # Extract coordinates for left and right eyes\n",
    "        left_eye = [(int(face_landmarks.landmark[i].x * w),\n",
    "                     int(face_landmarks.landmark[i].y * h)) for i in left_eye_indexes]\n",
    "        right_eye = [(int(face_landmarks.landmark[i].x * w),\n",
    "                      int(face_landmarks.landmark[i].y * h)) for i in right_eye_indexes]\n",
    "\n",
    "        l_ear = eye_aspect_ratio(left_eye_indexes,left_eye)\n",
    "        r_ear = eye_aspect_ratio(right_eye_indexes,right_eye)\n",
    "\n",
    "        EAR_THRESHOLD_S = 0.1 # threshold for closing eyes\n",
    "        EAR_THRESHOLD_M = 0.18 # threshold for closing eyes\n",
    "        if l_ear<=EAR_THRESHOLD_S or r_ear<=EAR_THRESHOLD_S:\n",
    "            gaze_direction='LD'\n",
    "        elif l_ear<=EAR_THRESHOLD_M or r_ear<=EAR_THRESHOLD_M:\n",
    "            gaze_direction='LUM'\n",
    "        else:\n",
    "            gaze_direction='LU'\n",
    "\n",
    "        annotated_image = cv2.resize(annotated_image, (512, 512))\n",
    "\n",
    "        # Annotate the image\n",
    "        # cv2.putText(annotated_image, gaze_direction, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(annotated_image, gaze_direction, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # return gaze_direction\n",
    "        # return f\"{gaze_direction}, {l_ear}, {r_ear}\"\n",
    "        return gaze_direction\n",
    "\n",
    "    # return annotated_image\n",
    "\n",
    "# Load an example image\n",
    "# image_path = \"/content/detected_faces/face_2.jpg\"\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# Detect eye pose\n",
    "# output_image = detect_eye_pose(image)\n",
    "\n",
    "# Display the result\n",
    "# cv2_imshow(output_image)\n",
    "# cv2.imshow(\"Eye Pose Detection\", output_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8a173-8815-44ce-b13c-c006e7d427ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f7bda",
   "metadata": {},
   "source": [
    "<h1>Head Tilt</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Camera matrix (assume focal length = width of the image for simplicity)\n",
    "def get_camera_matrix(image):\n",
    "    height, width = image.shape[:2]\n",
    "    focal_x = width\n",
    "    focal_y = height\n",
    "    center = (width, height)\n",
    "    # center = (width / 3, height / 5)\n",
    "    return np.array([\n",
    "        [focal_x, 0, center[0]],\n",
    "        [0, focal_x, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "# Define a function to classify head pose\n",
    "def classify_head_pose(pitch, yaw, roll, img_c, face_c):\n",
    "    # if abs(yaw) < 10 and abs(pitch) < 10:\n",
    "    if abs(yaw) < 10 and abs(pitch) < 10:\n",
    "        # return \"Looking Straight\"\n",
    "        return \"LS\"\n",
    "    elif yaw > 10:\n",
    "        # return \"Looking Left\"\n",
    "        if face_c>=img_c:\n",
    "            return \"LS\"\n",
    "        return \"LL\"\n",
    "    elif yaw < -10:\n",
    "        # return \"Looking Right\"\n",
    "        if face_c <=img_c:\n",
    "            return \"LS\"\n",
    "        return \"LR\"\n",
    "    elif pitch > 10:\n",
    "        # return \"Looking Down\"\n",
    "        return \"LD\"\n",
    "    elif pitch < -10:\n",
    "        # return \"Looking Up\"\n",
    "        return \"LU\"\n",
    "    # return \"Uncertain\"\n",
    "    return \"U\"\n",
    "\n",
    "\n",
    "# Initialize Mediapipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh \n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True,min_detection_confidence=0.02)\n",
    "\n",
    "def head_pose_classifier(faceid,image, img_c, face_c):\n",
    "\n",
    "  # Load the image\n",
    "  # image = cv2.imread(image_path)\n",
    "\n",
    "  if image is None:\n",
    "      raise ValueError(\"Image not found.\")\n",
    "  rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # Detect face mesh\n",
    "  results = face_mesh.process(rgb_image)\n",
    "\n",
    "  if results.multi_face_landmarks:\n",
    "  \n",
    "      for face_landmarks in results.multi_face_landmarks:\n",
    "          # Define 3D model points (generic face)\n",
    "          model_points = np.array([\n",
    "              (0.0, 0.0, 0.0),             # Nose tip\n",
    "              (0.0, -330.0, -65.0),        # Chin\n",
    "              (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "              (225.0, 170.0, -135.0),      # Right eye right corner\n",
    "              (-150.0, -150.0, -125.0),    # Left mouth corner\n",
    "              (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "          ], dtype=\"double\")\n",
    "\n",
    "          # Extract 2D image points from Mediapipe landmarks\n",
    "          landmarks = face_landmarks.landmark\n",
    "          image_points = np.array([\n",
    "              (landmarks[1].x * image.shape[1], landmarks[1].y * image.shape[0]),  # Nose tip\n",
    "              (landmarks[152].x * image.shape[1], landmarks[152].y * image.shape[0]),  # Chin\n",
    "              (landmarks[33].x * image.shape[1], landmarks[33].y * image.shape[0]),  # Left eye\n",
    "              (landmarks[263].x * image.shape[1], landmarks[263].y * image.shape[0]),  # Right eye\n",
    "              (landmarks[287].x * image.shape[1], landmarks[287].y * image.shape[0]),  # Left mouth corner\n",
    "              (landmarks[57].x * image.shape[1], landmarks[57].y * image.shape[0])   # Right mouth corner\n",
    "          ], dtype=\"double\")\n",
    "\n",
    "          # SolvePnP to estimate head pose\n",
    "          camera_matrix = get_camera_matrix(image)\n",
    "          dist_coeffs = np.zeros((4, 1))  # Assume no distortion\n",
    "          success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "              model_points, image_points, camera_matrix, dist_coeffs\n",
    "          )\n",
    "\n",
    "          # Convert rotation vector to Euler angles (pitch, yaw, roll)\n",
    "          rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "          pose_matrix = np.hstack((rotation_matrix, translation_vector))\n",
    "          _, _, _, _, _, _, euler_angles = cv2.decomposeProjectionMatrix(pose_matrix)\n",
    "          pitch, yaw, roll = euler_angles.flatten()\n",
    "\n",
    "          # Classify head pose\n",
    "          head_pose = classify_head_pose(pitch, yaw, roll, img_c, face_c)\n",
    "\n",
    "         \n",
    "          # return f\"Pitch: {pitch:.2f}, Yaw: {yaw:.2f}, Roll: {roll:.2f} -> {head_pose}\"\n",
    "          # print(f\"***{faceid} Pitch: {pitch:.2f}, Yaw: {yaw:.2f}, Roll: {roll:.2f} -> {head_pose}\")\n",
    "          # image = cv2.resize(image, (64, 64))\n",
    "\n",
    "          # Annotate the image with the classification\n",
    "          # cv2.putText(image, head_pose, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "          # cv2.putText(image, f\"Pitch: {pitch:.2f}, Yaw: {yaw:.2f}\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "          # cv2.imshow(image)\n",
    "          return head_pose,pitch,yaw,roll\n",
    "      return None, 0,0,0\n",
    "    \n",
    "  else:\n",
    "      # print(f\"***{faceid}\")\n",
    "      return None, 0,0,0\n",
    "\n",
    "      \n",
    "\n",
    "  # Display the image\n",
    "  # cv2.imshow(\"Head Pose Classification\", image)\n",
    "  # cv2_imshow(image)\n",
    "  # cv2.waitKey(0)\n",
    "  # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f14ece-8427-40b5-af35-677ed23e0313",
   "metadata": {},
   "source": [
    "# Detect, crop and save faces in frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def count_faces(input_image_path,df,img, backend):\n",
    "    image = cv2.imread(input_image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not found. Please check the path.\")\n",
    "\n",
    "    # Convert the image to RGB (MTCNN requires RGB input)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize the MTCNN detector\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector.detect_faces(rgb_image)\n",
    "    hc,wc = rgb_image.shape[:2] \n",
    "    # wc=wc//2\n",
    "    # a,b,_ = rgb_image.shape\n",
    "    # print(a,b)\n",
    "\n",
    "    # f={}\n",
    "\n",
    "    for i,face in enumerate(faces):\n",
    "        x, y, w, h = face['box']\n",
    "        # print(\"dim\",x,y,w,h)\n",
    "    \n",
    "        cv2.rectangle(rgb_image, (x-7, y-7), (x + w+7, y + h+7), (0, 0, 255), 2)\n",
    "        # Crop face  - enlarged the crop area for bettr head pose detection\n",
    "        face_crop = rgb_image[y-7:y + h+7, x-7:x + w+7]\n",
    "        # faces = []\n",
    "        name = f\"face_{i}\"\n",
    "        # faces.append(name)\n",
    "\n",
    "        # face_crop = image[y:y + h, x:x + w]\n",
    "        # p1 = detect_emotion_opencv(face_crop)\n",
    "        p1 = detect_emotion_mtcnn(face_crop,backend)\n",
    "        fc=x+w//2\n",
    "        \n",
    "        p2,p,yaw,r= head_pose_classifier(name,face_crop,wc//2,fc )\n",
    "        p3 = detect_eye_pose(face_crop)\n",
    "\n",
    "        d = [input_image_path,name,p1,p2,p3,x,y,w,h]\n",
    "#   \n",
    "        df.loc[len(df)] = d\n",
    "        # f.update({name:d})\n",
    "        # cv2.putText(rgb_image,f'{name}{p1} {p2} {p3}', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "      # for head pose\n",
    "        # cv2.putText(rgb_image,f'{name}{p2} p{p:.2f} y{yaw:.2f} r{r:.2f}', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "\n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe7069-ad4e-437c-8772-4eaaa462fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# your code here    \n",
    "# def consolidated_emotion_score()\n",
    "\n",
    "def multimodal_counts(path,fr,backend):\n",
    "    df = pd.DataFrame(columns=[\"Frame\",\"Face\", \"Emotion\",\"HeadTilt\",\"EyeGaze\",\"x\",\"y\",\"w\",\"h\"])\n",
    "    # processed_frame = preprocess(path)print(os.listdir())\n",
    "    # Every 5 minutes\n",
    "    start = time.process_time()\n",
    "    frame = count_faces(path,df,fr,backend)\n",
    "    name=f'./processed_3rd/{fr}.jpg'\n",
    "    # print(name,frame)\n",
    "    cv2.imwrite(name,frame)\n",
    " \n",
    "    return df, time.process_time() - start\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36982e23-6d77-4ed6-bc71-0fde7513cfe7",
   "metadata": {},
   "source": [
    "<h1>TOPSIS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0946390-62c6-4fc1-abd3-e8c6262c770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def multimodal_backend(backend, num_frames):\n",
    "    emotion_rank = {'neutral':6, 'happy':4, 'sad':5, 'angry':3,'surprise':4, 'disgust':2, 'fear':2}\n",
    "    head_rank = {'LL':3,'LR':3,'LU':4,'LD':1,'LS':5,None:2}\n",
    "    eye_rank = {'LD':2,'LUM':3,'LU':4,'No face detected.':1}\n",
    "        \n",
    "    weights = {\"Emotion\":(1/3),\"HeadTilt\":(1/3),\"EyeGaze\":(1/3)}\n",
    "    print(f\"----{backend}----\")\n",
    "    conc_indices=[]\n",
    "    times=[]\n",
    "    emotions=[]\n",
    "    head_tilt=[]\n",
    "    eyes=[]\n",
    "    df_all=pd.DataFrame(columns=[\"Frame\",\"Face\", \"Emotion\",\"HeadTilt\",\"EyeGaze\",\"x\",\"y\",\"w\",\"h\",\"EmotionRank\",\"HeadTiltRank\",\"EyeGazeRank\",\n",
    "            \"square_sum1\",'square_sum2','square_sum3','EmotionNormalised','HeadTiltNormalised','EyeGazeNormalised',\n",
    "           'a_Vi+','a_Vi-','b_Vi+','b_Vi-','c_Vi+','c_Vi-','Si+','Si-','Pi','Engaged/Disengaged','Rank'])\n",
    "    for i,path in enumerate(frames[:num_frames]):\n",
    "        df,t=multimodal_counts(path,f'frame{i}',backend)\n",
    "        print(f\">>Frame {i}\")\n",
    "        \n",
    "        times.append(t)\n",
    "        emotions.append(df['Emotion'].value_counts())\n",
    "        head_tilt.append(df['HeadTilt'].value_counts())\n",
    "        eyes.append(df['EyeGaze'].value_counts())\n",
    "        # print(f'Concentration Index frame {i} {ci}')\n",
    "    \n",
    "\n",
    "        # df,_ = multimodal_counts('f1.jpg','frame1','mtcnn')\n",
    "        df[\"EmotionRank\"] = df[\"Emotion\"].map(lambda x: emotion_rank[x])\n",
    "        df[\"HeadTiltRank\"] = df[\"HeadTilt\"].map(lambda x: head_rank[x])\n",
    "        df[\"EyeGazeRank\"] = df[\"EyeGaze\"].map(lambda x: eye_rank[x])\n",
    "        \n",
    "        # df['square_sum1'] = df['EmotionRank'].map(lambda x: x**2)\n",
    "        df['square_sum1'] = df['EmotionRank'].map(lambda x: x**2)\n",
    "        df['square_sum2'] = df['HeadTiltRank'].map(lambda x: x**2)\n",
    "        df['square_sum3'] = df['EyeGazeRank'].map(lambda x: x**2)\n",
    "        \n",
    "        s1 = (df['square_sum1'].sum())**0.5\n",
    "        s2 = (df['square_sum2'].sum())**0.5\n",
    "        s3 = (df['square_sum3'].sum())**0.5\n",
    "        print(s1,s2,s3)\n",
    "            \n",
    "        def vector_normalisation(x):\n",
    "            s=0\n",
    "            s+=x**2\n",
    "            return (x/s**0.5)\n",
    "        \n",
    "        # df = df[['Frame','Face','EmotionRank','HeadTiltRank','EyeGazeRank']].copy()\n",
    "        \n",
    "        df[\"EmotionNormalised\"]=df[\"EmotionRank\"].map(lambda x: x/s1)*weights[\"Emotion\"]\n",
    "        df[\"HeadTiltNormalised\"]=df[\"HeadTiltRank\"].map(lambda x: x/s2)*weights[\"HeadTilt\"]\n",
    "        df[\"EyeGazeNormalised\"]=df[\"EyeGazeRank\"].map(lambda x: x/s3)*weights[\"EyeGaze\"]\n",
    "        \n",
    "        # Calculating ideal (best, worst) values\n",
    "        ideal_best_Emotion = df[\"EmotionNormalised\"].max()\n",
    "        ideal_worst_Emotion = df[\"EmotionNormalised\"].min()\n",
    "        ideal_best_Head = df[\"HeadTiltNormalised\"].max()\n",
    "        ideal_worst_Head = df[\"HeadTiltNormalised\"].min()\n",
    "        ideal_best_EyeGaze = df[\"EyeGazeNormalised\"].max()\n",
    "        ideal_worst_EyeGaze = df[\"EyeGazeNormalised\"].min()\n",
    "        \n",
    "        # print(f\"Emotion: {ideal_best_Emotion},{ideal_worst_Emotion}\")\n",
    "        # print(f\"Head: {ideal_best_Head},{ideal_worst_Head}\")\n",
    "        # print(f\"Eye: {ideal_best_EyeGaze},{ideal_worst_EyeGaze}\")\n",
    "        \n",
    "        df['a_Vi+'] = df[\"EmotionNormalised\"].max()\n",
    "        df['a_Vi-'] = df[\"EmotionNormalised\"].min()\n",
    "        df['b_Vi+'] = df[\"HeadTiltNormalised\"].max()\n",
    "        df['b_Vi-'] = df[\"HeadTiltNormalised\"].min()\n",
    "        df['c_Vi+'] = df[\"EyeGazeNormalised\"].max()\n",
    "        df['c_Vi-'] = df[\"EyeGazeNormalised\"].min()\n",
    "        \n",
    "        # Euclidean distance from ideal best (Si+) and ideal worst (Si-)\n",
    "        def euclidean_distance(a,b,c):\n",
    "            return ((a-ideal_best_Emotion)**2+(b-ideal_best_Head)**2+(c-ideal_best_EyeGaze)**2)**0.5\n",
    "        \n",
    "        df['Si+'] = df.apply(lambda row: ((row['EmotionNormalised']-ideal_best_Emotion)**2+(row['HeadTiltNormalised']-ideal_best_Head)**2+(row['EyeGazeNormalised']-ideal_best_EyeGaze)**2)**0.5,axis=1)\n",
    "        df['Si-'] = df.apply(lambda row: ((row['EmotionNormalised']-ideal_worst_Emotion)**2+(row['HeadTiltNormalised']-ideal_worst_Head)**2+(row['EyeGazeNormalised']-ideal_worst_EyeGaze)**2)**0.5,axis=1)\n",
    "        \n",
    "        # Calculate performance score (Pi)\n",
    "        def score(a,b):\n",
    "            return (b/(a+b))\n",
    "        \n",
    "        df['Pi'] = df.apply(lambda row: row['Si-']/(row['Si+']+row['Si-']),axis = 1)\n",
    "        def classifier(x):\n",
    "            if x<0.25:\n",
    "                return 0\n",
    "            elif x>=0.25 and x<0.5:\n",
    "                return 1\n",
    "            elif x>=0.5 and x<0.75:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "\n",
    "        # df['Engaged/Disengaged'] = df['Pi_1'].apply(lambda x: 'e' if x>=0.5 else 'ne')\n",
    "        df['Engaged/Disengaged'] = df['Pi'].apply(classifier)\n",
    "        \n",
    "        df['Rank'] = df['Pi'].rank(method='max',ascending=True)\n",
    "    \n",
    "        # df_all=df_all.append(df)\n",
    "        df_all = pd.concat([df_all,df])\n",
    "        \n",
    "        conc_indices.append(df['Pi'].sum())\n",
    "\n",
    "        \n",
    "    \n",
    "    return df_all,conc_indices,times,emotions, head_tilt,eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32559412-8ee5-4811-9eae-f020e165ebc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_frames=17*3\n",
    "# num_frames = 10\n",
    "# conc_indices_mtcnn,times_mtcnn,emotions_mtcnn,hp_mtcnn, eyes_mtcnn = multimodal_backend(\"mtcnn\", num_frames)\n",
    "df_oc, conc_indices_oc,times_oc,emotions_oc,hp_oc,eyes_oc = multimodal_backend(\"opencv\",num_frames)\n",
    "# df_oc.to_csv('df_ofc.csv')\n",
    "# conc_indices_dl,times_dl,emotions_dl = emotion_backend(\"dlib\")\n",
    "# conc_indices_mp,times_mp,emotions_mp, hp_mp, eyes_mp = multimodal_backend(\"mediapipe\",num_frames)\n",
    "# conc_indices_rf,times_rf,emotions_rf = emotion_backend(\"retinafce\")\n",
    "# conc_indices_oc,times_oc,emotions_oc = multimodal_backend(\"ArcFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8430f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ee6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# box_color = \n",
    "\n",
    "box = pd.read_csv('df_ofc.csv')\n",
    "\n",
    "# if box['Pi'] < 0.2:\n",
    "#     box_color = (255,0,0)\n",
    "# elif box['Pi'] >= 0.2 and box['Pi'] < 0.4:\n",
    "#     box_color = (255,167,0)\n",
    "# elif box['Pi'] >= 0.4 and box['Pi'] < 0.6:\n",
    "#     box_color = (255,244,0)\n",
    "# elif box['Pi'] >= 0.6 and box['Pi'] < 0.8:\n",
    "#     box_color = (163,255,0)\n",
    "# else:\n",
    "#     box_color = (44,186,0)\n",
    "\n",
    "\n",
    "def box_color(a):\n",
    "    if a < 0.2:\n",
    "        return (0,0,255)\n",
    "    elif a >= 0.2 and a < 0.4:\n",
    "        return (0,167,255)\n",
    "    elif a >= 0.4 and a < 0.6:\n",
    "        return (0,244,255)\n",
    "    elif a >= 0.6 and a < 0.8:\n",
    "        return (0,255,163)\n",
    "    else:\n",
    "        return (0,186,44)\n",
    "    \n",
    "frame_names = box['Frame'].unique()\n",
    "\n",
    "# print(len())\n",
    "\n",
    "print(frame_names, box_color)\n",
    "for k,f in enumerate(frame_names):\n",
    "    img_this_context = cv2.imread(f)\n",
    "    for i, val in box.iterrows():\n",
    "#     print(i, j['Frame'])\n",
    "        if val['Frame']==f:\n",
    "            b = box_color(val['Pi'])\n",
    "            cv2.rectangle(img_this_context, (val['x']-7, val['y']-7), (val['x'] + val['w']+7, val['y'] + val['h']+7), b, 2)\n",
    "            # cv2.putText(img_this_context,f\"{val['Face']} {val['Emotion']} {val['HeadTilt']} {val['EyeGaze']}\", (val['x'], val['y']), \n",
    "            #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "            cv2.putText(img_this_context,f\"{val['Face']} {val['Emotion']} {val['HeadTilt']} {val['EyeGaze']}\", (val['x'], val['y']), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 255, 255), 1)\n",
    "    name=f'./processed_3rd/frame{k}.jpg'\n",
    "    # print(name,frame)\n",
    "    try:\n",
    "        print(conc_indices_oc[k])\n",
    "        cv2.putText(img_this_context,f\"ACI: {n_conc_indices_oc[k]*100:.2f}%\", (1500, 100), cv2.FONT_HERSHEY_DUPLEX, 2,b, 1)\n",
    "    except:\n",
    "        break\n",
    "    # cv2.putText(img_this_context,f\"{conc_indices_oc[k]}\", (1880, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2,(255, 255, 255), 1)\n",
    "    cv2.imwrite(name,img_this_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8068c4e-d992-4ab0-9b11-66a14c9d0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old output\n",
    "\n",
    "# ['./data_3rd/frame1_1798.jpg' './data_3rd/frame2_1799.jpg'\n",
    "#  './data_3rd/frame3_1800.jpg' './data_3rd/frame4_3598.jpg'\n",
    "#  './data_3rd/frame5_3599.jpg' './data_3rd/frame6_3600.jpg'\n",
    "#  './data_3rd/frame7_5398.jpg' './data_3rd/frame8_5399.jpg'\n",
    "#  './data_3rd/frame9_5400.jpg' './data_3rd/frame10_7198.jpg'\n",
    "#  './data_3rd/frame11_7199.jpg' './data_3rd/frame12_7200.jpg'\n",
    "#  './data_3rd/frame13_8998.jpg' './data_3rd/frame14_8999.jpg'\n",
    "#  './data_3rd/frame15_9000.jpg' './data_3rd/frame16_10798.jpg'\n",
    "#  './data_3rd/frame17_10799.jpg' './data_3rd/frame18_10800.jpg'\n",
    "#  './data_3rd/frame19_12598.jpg' './data_3rd/frame20_12599.jpg'\n",
    "#  './data_3rd/frame21_12600.jpg' './data_3rd/frame22_14398.jpg'\n",
    "#  './data_3rd/frame23_14399.jpg' './data_3rd/frame24_14400.jpg'\n",
    "#  './data_3rd/frame25_16198.jpg' './data_3rd/frame26_16199.jpg'\n",
    "#  './data_3rd/frame27_16200.jpg' './data_3rd/frame28_17998.jpg'\n",
    "#  './data_3rd/frame29_17999.jpg' './data_3rd/frame30_18000.jpg'\n",
    "#  './data_3rd/frame31_19798.jpg' './data_3rd/frame32_19799.jpg'\n",
    "#  './data_3rd/frame33_19800.jpg' './data_3rd/frame34_21598.jpg'\n",
    "#  './data_3rd/frame35_21599.jpg' './data_3rd/frame36_21600.jpg'\n",
    "#  './data_3rd/frame37_23398.jpg' './data_3rd/frame38_23399.jpg'\n",
    "#  './data_3rd/frame39_23400.jpg' './data_3rd/frame40_25198.jpg'\n",
    "#  './data_3rd/frame41_25199.jpg' './data_3rd/frame42_25200.jpg'\n",
    "#  './data_3rd/frame43_26998.jpg' './data_3rd/frame44_26999.jpg'\n",
    "#  './data_3rd/frame45_27000.jpg' './data_3rd/frame46_28798.jpg'\n",
    "#  './data_3rd/frame47_28799.jpg' './data_3rd/frame48_28800.jpg'\n",
    "#  './data_3rd/frame49_30598.jpg' './data_3rd/frame50_30599.jpg'\n",
    "#  './data_3rd/frame51_30600.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conc_indices_oc,len(conc_indices_oc))\n",
    "# print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43742f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def calc_normalized_ci(data):\n",
    "    #count faces in each frame \n",
    "    d1 = data.groupby(['Frame']).size().reset_index(name='count faces')\n",
    "    hm=statistics.harmonic_mean(d1['count faces'])\n",
    "    print(hm)\n",
    "    \n",
    "    n_conc_indices=[min(1,item/hm) for item in conc_indices_oc]\n",
    "    return n_conc_indices\n",
    "          \n",
    "n_conc_indices_oc=calc_normalized_ci(df_oc)\n",
    "print(n_conc_indices_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(times_oc[:17*3])\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ccdb4-0671-48e2-aeaa-e36d8a2971bb",
   "metadata": {},
   "source": [
    "<h1>Average of 3fp per min</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized\n",
    "def conc_norm(ci_frames):\n",
    "    norm_ci_25min=[]\n",
    "    for i in range(0,len(ci_frames),3):\n",
    "        norm_ci_25min.append(sum(ci_frames[i:i+3])/3)\n",
    "    return norm_ci_25min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c949cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cn_mtcnn = conc_norm(conc_indices_mtcnn)\n",
    "# cn_mp = conc_norm(conc_indices_mp)\n",
    "cn_oc = conc_norm(n_conc_indices_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing packages \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# loading dataset \n",
    "  \n",
    "# draw lineplot \n",
    "\n",
    "lp=sns.lineplot( data=cn_oc) \n",
    "lp.set(xlabel='Time (minutes)',ylabel='concentration_index', title='Concentration index during a lecture (Time Horizon ≈ 17 min)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "sns.set(style=\"ticks\")  \n",
    "sns.color_palette(\"rocket\")\n",
    "# loading dataset \n",
    "  \n",
    "# draw lineplot \n",
    "lp=sns.lineplot( data=cn_mtcnn)\n",
    "sns.lineplot( data=cn_mp)\n",
    "sns.lineplot( data=cn_oc)\n",
    "lp.set(xlabel='Time (minutes)',ylabel='concentration_index', title='Concentration index during a lecture (Time Horizon ≈ 17 min)')\n",
    "plt.legend(title= \"Deepface Backend\", labels = [\"MTCNN\", \"Mediapipe\", \"OpenCV\"], loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series(norm_ci_25min[:17])\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'img.jpg'\n",
    "\n",
    "# Reading an image in default mode\n",
    "image = cv2.imread(path)\n",
    "window_name = 'image'\n",
    "cv2.imshow(window_name, image)\n",
    "\n",
    "# waits for user to press any key\n",
    "# (this is necessary to avoid Python kernel form crashing)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# closing all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9927b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc= {\"happy\":0, \"sad\":0,\"neutral\":0,\"angry\":0,\"disgust\":0,\"surprise\":0,\"fear\":0}\n",
    "em_all=[\"happy\", \"sad\",\"neutral\",\"angry\",\"disgust\",\"surprise\",\"fear\"]\n",
    "for e in emotions_oc:\n",
    "    for em in em_all:\n",
    "        mc[em]+=e.get(em,0)\n",
    "print(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50e4df-ea2b-4540-b6fe-b6572f974141",
   "metadata": {},
   "source": [
    "# Frame to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc0e5-418f-4e01-82df-842deb46df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "image_folder = 'processed_3rd'\n",
    "video_name = 'video.avi'\n",
    "\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
    "\n",
    "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, 0, 1, (width,height))\n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
